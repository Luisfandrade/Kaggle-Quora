{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Quora Toxic Questions Competition\nby Luis Andrade\n\n**Background**\n\nQuora is a platform that empowers people to learn from each other. On Quora, people can ask questions and connect with others who contribute unique insights and quality answers. A key challenge is to weed out insincere questions -- those founded upon false premises, or that intend to make a statement rather than look for helpful answers.\n\n**Goal**\n\nThe goal of this competition was to develop models that identify and flag insincere questions. \n\n**Keywords**\n* NLP\n* Word embeddings\n* Deep Learning\n* Binary classification\n\n**Notebook Structure**\n1. Data loading and preprocessing\n2. Embeddings creation\n3. Model creation\n4. Trainnig & evaluation\n5. Submission"},{"metadata":{"trusted":true,"_uuid":"cbbc124c709af5623a77ff241659ab4910f4576d"},"cell_type":"code","source":"import os\nimport time\nimport numpy as np \nimport pandas as pd \nfrom tqdm import tqdm\nimport math\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, CuDNNLSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D\nfrom keras.layers import Bidirectional, GlobalMaxPool1D, GlobalAveragePooling1D\nfrom keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D, concatenate\nfrom keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D\nfrom keras.optimizers import Adam\nfrom keras.models import Model\nfrom keras import backend as K\nfrom keras.engine.topology import Layer\nfrom keras import initializers, regularizers, constraints, optimizers, layers\n\n%matplotlib inline","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# list available files\nprint(os.listdir(\"../input\"))\nprint(os.listdir(\"../input/embeddings\"))\n","execution_count":4,"outputs":[{"output_type":"stream","text":"['train.csv', 'test.csv', 'sample_submission.csv', 'embeddings']\n['GoogleNews-vectors-negative300', 'wiki-news-300d-1M', 'paragram_300_sl999', 'glove.840B.300d']\n","name":"stdout"}]},{"metadata":{"trusted":true,"_uuid":"12546f3875236f417b3793cfd2a584eb1cd05907"},"cell_type":"code","source":"# some config values \nembed_size = 300 # how big is each word vector\nmax_features = 120000 # how many unique words to use (i.e num rows in embedding vector)\nmaxlen = 70 # max number of words in a question to use","execution_count":5,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data loading & preprocessing"},{"metadata":{"trusted":true,"_uuid":"206218cbd6aecc4ef1d15887ea6864c94f017dbd"},"cell_type":"code","source":"puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n\ndef clean_text(x):\n    for punct in puncts:\n        if punct in x:\n            x = x.replace(punct, f' {punct} ')\n    return x","execution_count":18,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"74bfe9372fc41a3ad0bea93b08d9cdeb1cfcb7f2"},"cell_type":"code","source":"def load_and_prec():\n    \"\"\"Function to load the data and preprocess it\"\"\"\n    train_df = pd.read_csv(\"../input/train.csv\")\n    test_df = pd.read_csv(\"../input/test.csv\")\n    \n    train_df[\"question_text\"] = train_df[\"question_text\"].str.lower()\n    test_df[\"question_text\"] = test_df[\"question_text\"].str.lower()\n    \n    train_df[\"question_text\"] = train_df[\"question_text\"].apply(lambda x: clean_text(x))\n    test_df[\"question_text\"] = test_df[\"question_text\"].apply(lambda x: clean_text(x))\n    \n    print(\"Train shape : \",train_df.shape)\n    print(\"Test shape : \",test_df.shape)\n    \n    ## split to train and val\n    train_df, val_df = train_test_split(train_df, test_size=0.001, random_state=2018) # hahaha\n\n\n    ## fill up the missing values\n    train_X = train_df[\"question_text\"].fillna(\"_##_\").values\n    val_X = val_df[\"question_text\"].fillna(\"_##_\").values\n    test_X = test_df[\"question_text\"].fillna(\"_##_\").values\n    \n    ## Tokenize the sentences\n    tokenizer = Tokenizer(num_words=max_features)\n    tokenizer.fit_on_texts(list(train_X))\n    train_X = tokenizer.texts_to_sequences(train_X)\n    val_X = tokenizer.texts_to_sequences(val_X)\n    test_X = tokenizer.texts_to_sequences(test_X)\n\n    ## Pad the sentences \n    train_X = pad_sequences(train_X, maxlen=maxlen)\n    val_X = pad_sequences(val_X, maxlen=maxlen)\n    test_X = pad_sequences(test_X, maxlen=maxlen)\n\n    ## Get the target values\n    train_y = train_df['target'].values\n    val_y = val_df['target'].values  \n    \n    #shuffle the data\n    np.random.seed(2018)\n    trn_idx = np.random.permutation(len(train_X))\n    val_idx = np.random.permutation(len(val_X))\n    \n    train_X = train_X[trn_idx]\n    val_X = val_X[val_idx]\n    train_y = train_y[trn_idx]\n    val_y = val_y[val_idx]    \n\n    return train_X, val_X, test_X, train_y, val_y, tokenizer.word_index","execution_count":7,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Embedding Creation"},{"metadata":{"trusted":true,"_uuid":"5cb5964ad6fd7eded18588aae6f1d628c5b8e342"},"cell_type":"code","source":"def get_embeddings(embeddings_index, word_index, name):\n    \"\"\"Compute the embeddings matrix\"\"\"\n    all_embs = np.stack(embeddings_index.values())\n    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n    embed_size = all_embs.shape[1]\n\n    nb_words = min(max_features, len(word_index))\n    # Warm-start embedding matrix with normal distribution with same mean \n    # and standard deviation as the pre-trained embeddings \n    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n    \n    for word, i in word_index.items():\n        if i >= max_features: continue\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None: \n            embedding_matrix[i] = embedding_vector\n            \n    return embedding_matrix ","execution_count":8,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7b77dea47799ce83c9719f96818438171b61b524"},"cell_type":"code","source":"def load_glove(word_index):\n    EMBEDDING_FILE = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'\n    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n    \n    return get_embeddings(embeddings_index, word_index, \"glove\")\n","execution_count":9,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7ce382585540ca47d3162567a95d2a95012b4ffb"},"cell_type":"code","source":"def load_fasttext(word_index):    \n    EMBEDDING_FILE = '../input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'\n    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n    embeddings_index = dict(get_coefs(*o.split(\" \"))\n                            for o in open(EMBEDDING_FILE) if len(o)>100)\n\n    return get_embeddings(embeddings_index, word_index, \"fasttext\")","execution_count":10,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9949c9ff931d5383babc406b7656081ce3a27cf5"},"cell_type":"code","source":"def load_para(word_index):\n    EMBEDDING_FILE = '../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt'\n    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n    embeddings_index = dict(get_coefs(*o.split(\" \")) \n                            for o in open(EMBEDDING_FILE, encoding=\"utf8\",\n                                          errors='ignore')\n                            if len(o)>100)\n    return get_embeddings(embeddings_index, word_index, \"para\")","execution_count":11,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Creation\n\nIn this section different deep learning models are created"},{"metadata":{"trusted":true,"_uuid":"95f228b7a2b0450887e0f3ab14776fefbd66f079"},"cell_type":"code","source":"def model_cnn(embedding_matrix):\n    \"\"\"CNN model\"\"\"\n    filter_sizes = [1,2,3,5]\n    num_filters = 36\n\n    inp = Input(shape=(maxlen,))\n    x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n    x = Reshape((maxlen, embed_size, 1))(x)\n    \n    maxpool_pool = []\n    for f_size in filter_sizes:\n        conv = Conv2D(num_filters, kernel_size=(f_size, embed_size),\n                      kernel_initializer='he_normal', activation='relu')(x)\n        maxpool_pool.append(MaxPool2D(pool_size=(maxlen - f_size + 1, 1))(conv))\n        \n    z = Concatenate(axis=1)(maxpool_pool)   \n    z = Flatten()(z)\n    z = Dropout(0.1)(z)\n\n    outp = Dense(1, activation=\"sigmoid\")(z)\n\n    model = Model(inputs=inp, outputs=outp)\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    \n    return model","execution_count":12,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"938a83e1273b017e755e3c7133de5ead8f2f167d"},"cell_type":"code","source":"class Attention(Layer):\n    \"\"\"Attention Layer\"\"\"\n    def __init__(self, step_dim,\n                 W_regularizer=None, b_regularizer=None,\n                 W_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.bias = bias\n        self.step_dim = step_dim\n        self.features_dim = 0\n        super(Attention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n\n        self.W = self.add_weight((input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        self.features_dim = input_shape[-1]\n\n        if self.bias:\n            self.b = self.add_weight((input_shape[1],),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n        else:\n            self.b = None\n\n        self.built = True\n\n    def compute_mask(self, input, input_mask=None):\n        return None\n\n    def call(self, x, mask=None):\n        features_dim = self.features_dim\n        step_dim = self.step_dim\n\n        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n\n        if self.bias:\n            eij += self.b\n\n        eij = K.tanh(eij)\n\n        a = K.exp(eij)\n\n        if mask is not None:\n            a *= K.cast(mask, K.floatx())\n\n        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n\n        a = K.expand_dims(a)\n        weighted_input = x * a\n        return K.sum(weighted_input, axis=1)\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0],  self.features_dim","execution_count":13,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9041d58aca74f0a4d4b795aef4a6b6e6df9c9ac1"},"cell_type":"code","source":"def model_lstm_atten(embedding_matrix):\n    \"\"\"Multi BiLSTM with Attention Layer\"\"\"\n    inp = Input(shape=(maxlen,))\n    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n    x = Bidirectional(CuDNNLSTM(128, return_sequences=True))(x)\n    x = Bidirectional(CuDNNLSTM(64, return_sequences=True))(x)\n    x = Attention(maxlen)(x)\n    x = Dense(64, activation=\"relu\")(x)\n    x = Dense(1, activation=\"sigmoid\")(x)\n    model = Model(inputs=inp, outputs=x)\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    \n    return model","execution_count":14,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d2b866db58978dc729c175d358b8f7cd4e8d117f"},"cell_type":"code","source":"def model_lstm_du(embedding_matrix):\n    \"\"\"BiLSTM with mixed pooling and dropout\"\"\"\n    inp = Input(shape=(maxlen,))\n    x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n    x = Bidirectional(CuDNNGRU(64, return_sequences=True))(x)\n    avg_pool = GlobalAveragePooling1D()(x)\n    max_pool = GlobalMaxPool1D()(x)\n    conc = concatenate([avg_pool, max_pool])\n    conc = Dense(64, activation=\"relu\")(conc)\n    conc = Dropout(0.1)(conc)\n    outp = Dense(1, activation=\"sigmoid\")(conc)\n    \n    model = Model(inputs=inp, outputs=outp)\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model","execution_count":15,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Trainning & Evaluation"},{"metadata":{"trusted":true,"_uuid":"1340ce2271f01b953af360fdd3fde8463ac06896"},"cell_type":"code","source":"def train_pred(model, epochs=2, verbose=0):\n    model.fit(train_X, train_y, batch_size=512, epochs=epochs,\n              validation_data=(val_X, val_y))\n    pred_val_y = model.predict([val_X], batch_size=1024, verbose=verbose)\n    pred_test_y = model.predict([test_X], batch_size=1024, verbose=verbose)\n    \n    \"\"\"\n     #Plot training & validation accuracy values\n     plt.plot(model.history['acc'])\n     plt.plot(model.history['val_acc'])\n     plt.title('Model accuracy')\n     plt.ylabel('Accuracy')\n     plt.xlabel('Epoch')\n     plt.legend(['Train', 'Test'], loc='upper left')\n     plt.show()\n\n     # Plot training & validation loss values\n     plt.plot(model.history.history['loss'])\n     plt.plot(model.history.history['val_loss'])\n     plt.title('Model loss')\n     plt.ylabel('Loss')\n     plt.xlabel('Epoch')\n     plt.legend(['Train', 'Test'], loc='upper left')\n     plt.show()\n     \n     \"\"\"\n    return pred_val_y, pred_test_y","execution_count":20,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"60530b3e6dcf75e3aa61fcf41c9e4920346ed86a"},"cell_type":"code","source":"train_X, val_X, test_X, train_y, val_y, word_index = load_and_prec()\nemb_matrix_glove = load_glove(word_index)\nprint(\"Done with glove\")\n#emb_matrix_para = load_para(word_index)\n#print(\"Done with paragram\")\n#embedding_matrix = np.mean([emb_matrix_glove, emb_matrix_para], axis = 0)\n#print(\"Finished embedding matrix\")","execution_count":17,"outputs":[{"output_type":"stream","text":"Train shape :  (1306122, 3)\nTest shape :  (375806, 2)\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:3: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n  This is separate from the ipykernel package so we can avoid doing imports until\n","name":"stderr"},{"output_type":"stream","text":"Done with glove\nDone with paragram\nFinished embedding matrix\n","name":"stdout"}]},{"metadata":{"trusted":true,"_uuid":"83ed9129671ceef3ad0993df2086b97559a9343a"},"cell_type":"code","source":"outputs = {}\n\nmodel1 = model_cnn(emb_matrix_glove)\npred_val_y, pred_test_y = train_pred(model1, epochs=3, verbose=3)\noutputs[\"cnn para\"] = (pred_val_y, pred_test_y)\n\nmodel2 = model_lstm_atten(emb_matrix_glove)\npred_val_y, pred_test_y = train_pred(model2, epochs=3, verbose=3)\noutputs[\"bilstm glove\"] = (pred_val_y, pred_test_y)\n\nmodel3 = model_lstm_du(emb_matrix_glove)\npred_val_y, pred_test_y = train_pred(model3, epochs=3, verbose=3)\noutputs[\"lstm maxpool mix\"] = (pred_val_y, pred_test_y)","execution_count":21,"outputs":[{"output_type":"stream","text":"Train on 1304815 samples, validate on 1307 samples\nEpoch 1/1\n1304815/1304815 [==============================] - 108s 82us/step - loss: 0.1149 - acc: 0.9550 - val_loss: 0.0942 - val_acc: 0.9549\nTrain on 1304815 samples, validate on 1307 samples\nEpoch 1/1\n1304815/1304815 [==============================] - 126s 97us/step - loss: 0.1146 - acc: 0.9550 - val_loss: 0.0948 - val_acc: 0.9579\nWARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nDeprecated in favor of operator or tf.math.divide.\nTrain on 1304815 samples, validate on 1307 samples\nEpoch 1/1\n1304815/1304815 [==============================] - 90s 69us/step - loss: 0.1095 - acc: 0.9568 - val_loss: 0.0929 - val_acc: 0.9633\n","name":"stdout"}]},{"metadata":{"trusted":true,"_uuid":"f34c5ba89f2f10c6a9d63a965c5cf798e592a356"},"cell_type":"code","source":"# \"Stack\" all model predictions by averaging over their outputs\npred_val_y = np.mean([output[0] for output in outputs.values()], axis=0)\n","execution_count":22,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"17a119a117fe912f3df7817cb62cc0817599fab1"},"cell_type":"code","source":"def find_best_threshold(pred_val_y, val_y):\n    thresholds = []\n    for thresh in np.arange(0.1, 0.501, 0.01):\n        thresh = np.round(thresh, 2)\n        res = metrics.f1_score(val_y, (pred_val_y > thresh).astype(int))\n        thresholds.append([thresh, res])\n        print(\"F1 score at threshold {0} is {1}\".format(thresh, res))\n\n    thresholds.sort(key=lambda x: x[1], reverse=True)\n    best_thresh = thresholds[0][0]\n    print(\"Best threshold: \", best_thresh)\n    return best_thresh","execution_count":23,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"665bfd4934ec638d0fcf7ed226e0ee5954d0fc6b"},"cell_type":"code","source":"# Find best threshold \nbest_thresh = find_best_threshold(pred_val_y, val_y)\npred_test_y = (pred_test_y > best_thresh).astype(int)\n","execution_count":24,"outputs":[{"output_type":"stream","text":"F1 score at threshold 0.1 is 0.6135458167330677\nF1 score at threshold 0.11 is 0.6178861788617885\nF1 score at threshold 0.12 is 0.6229508196721312\nF1 score at threshold 0.13 is 0.6244725738396625\nF1 score at threshold 0.14 is 0.6324786324786326\nF1 score at threshold 0.15 is 0.6406926406926408\nF1 score at threshold 0.16 is 0.6371681415929205\nF1 score at threshold 0.17 is 0.6371681415929205\nF1 score at threshold 0.18 is 0.6457399103139013\nF1 score at threshold 0.19 is 0.6425339366515838\nF1 score at threshold 0.2 is 0.6481481481481481\nF1 score at threshold 0.21 is 0.6635071090047393\nF1 score at threshold 0.22 is 0.6571428571428573\nF1 score at threshold 0.23 is 0.6570048309178744\nF1 score at threshold 0.24 is 0.6601941747572816\nF1 score at threshold 0.25 is 0.6699507389162562\nF1 score at threshold 0.26 is 0.6799999999999999\nF1 score at threshold 0.27 is 0.6868686868686867\nF1 score at threshold 0.28 is 0.6938775510204082\nF1 score at threshold 0.29 is 0.6871794871794871\nF1 score at threshold 0.3 is 0.6907216494845361\nF1 score at threshold 0.31 is 0.6943005181347152\nF1 score at threshold 0.32 is 0.6947368421052632\nF1 score at threshold 0.33 is 0.7021276595744681\nF1 score at threshold 0.34 is 0.7096774193548387\nF1 score at threshold 0.35 is 0.7135135135135137\nF1 score at threshold 0.36 is 0.7065217391304348\nF1 score at threshold 0.37 is 0.7103825136612022\nF1 score at threshold 0.38 is 0.718232044198895\nF1 score at threshold 0.39 is 0.7118644067796609\nF1 score at threshold 0.4 is 0.7045454545454545\nF1 score at threshold 0.41 is 0.6936416184971097\nF1 score at threshold 0.42 is 0.6745562130177514\nF1 score at threshold 0.43 is 0.6745562130177514\nF1 score at threshold 0.44 is 0.6666666666666667\nF1 score at threshold 0.45 is 0.654320987654321\nF1 score at threshold 0.46 is 0.6583850931677018\nF1 score at threshold 0.47 is 0.6583850931677018\nF1 score at threshold 0.48 is 0.6625000000000001\nF1 score at threshold 0.49 is 0.6708860759493671\nF1 score at threshold 0.5 is 0.6753246753246753\nBest threshold:  0.38\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Submission"},{"metadata":{"trusted":true,"_uuid":"a4dc01ee7d8f912a454a0581da6c72556b2a52e0"},"cell_type":"code","source":"test_df = pd.read_csv(\"../input/test.csv\", usecols=[\"qid\"])\nout_df = pd.DataFrame({\"qid\":test_df[\"qid\"].values})\nout_df['prediction'] = pred_test_y\nout_df.to_csv(\"submission.csv\", index=False)","execution_count":25,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}